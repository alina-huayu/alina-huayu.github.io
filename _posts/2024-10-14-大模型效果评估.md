---
layout: post
title: "如何评估大模型的效果？"
---

# 1. 明确评估目标：
- 更好的理解大模型的长处和短处，定位模型问题：通过评估，找到模型在当前场景所存在的问题，而后进行分类归因。
- 找到有效的提升方案：基于定位的问题及归因分析，来找到有效的解决方案（如优化Prompt、借助工程手段、微调模型、提升私域知识质量等）。
- 监控效果状态，防范未知和可能的风险：大模型存在随机波动的特点，比如A版本模型可以顺利使用的Prompt，可能到B版本就失效了。因此，我们要在每个模型版本上，都进行效果评估，防止出现“意料之外”的效果下降。
- 挖掘线上BadCase，优化应用效果：通过数据回流获得线上真实流量中的用户请求与应用返回结果（甚至针对多轮交互场景，跟踪收集每一轮次用户请求-响应数据） ，通过评估找到不符合预期的Case，并分析确定优化的方案。

---

# 2. 制定评估策略：
一个完整的评估策略需要包括：

- 评估指标：准确率、满意度、优劣程度等。
- 计分方式：正确/错误、三分制、GSB等
- 评估规则：每个场景都需要定义明确的评估规则，即每个评估指标的打分原则、每个分值代表的含义等，为人工或自动化评估提供参考。
- 评估标签：评估时评估人或评估模型需要通过评估标签来标记选择该分值的原因，通常只有打低分时需要标记。
![评估策略](/images/打分.jpg)

---

# 3. 确定评估的方式：
评估方式是与评估指标相关联的，即使是同一个场景的多个指标，也可能分人工评估和自动化评估两种方式，评估结束后需要将所有指标汇总分析。
![评估方式](/images/评估方式.jpg)

人工评估可以有两种方式：
- 分配协作评估：评估集分别分配给多人协作完成，每个Case由一个人进行评估打分，可能需要增加互审及验收环节。
- 背靠背评估：每个Case同时由多人进行打分，之后再由审核/验收人员查看Diff，进行决策

---

# 4. 构建评估集：
需要考虑来源、组成、难度、量级等多个因素。
- 来源：
![评估集的构建](/images/评估集的构建.jpeg)
- 组成：
评估集合要足够分散，覆盖不同的场景分支，评估结果才足够可靠。
- 难度：
确保评估集涵盖了不同难度的数据
- 量级：
评估集条数越多，能覆盖的类型越全，评估可信度越高（评估偏差越小） ，但成本也越高，建议量级如下：
![评估集数量建议](/images/评估集数量建议.jpeg)

---

# 5. 分析评估结果：
- 一方面是对评估指标的汇总统计分析。

    1.有效性判断：通常使用准确率来进行统计，即AI应用返回正确的case数据 / 所有评估的case数。

    2.满意度得分：通常使用平均分来进行统计，同时也会统计各个得分的占比。例如：1.15（三分制） ，其中2分占比40%，1分占比35%，0分占比25%。

    3.GSB评分：汇总单个Case的G、S、B，得出整体的GSB比值，例如：A VS B=50G:20S:30B ，同时也可以进一步计算 ΔGSB=(G-B)/(G+S+B)=0.2 ，可以认为A VS B是正向的（即A优于B）。

- 另一方面，需要对具体case进行深入分析，获取更多的评估细节。

    - 进行Case级的分析时，一般建议选择效果表现不好、相对历史评估结果发生变化、业务重点监控的核心Case等。

    - 在具体的Case分析时，可以借助评估过程中的备注、标签等信息；同时也可以参考该Case的历史表现，进而分析迭代过程中是否导致了Case的劣化。最终需要分析出Case具体的原因与解决方案，由研发后续优化。


