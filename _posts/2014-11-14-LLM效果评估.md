---
layout: post
title: "LLM的效果如何评估？"
---

# 1. 明确评估目标：
- 更好的理解大模型的长处和短处，定位模型问题：通过评估，找到模型在当前场景所存在的问题，而后进行分类归因。
- 找到有效的提升方案：基于定位的问题及归因分析，来找到有效的解决方案（如优化Prompt、借助工程手段、微调模型、提升私域知识质量等）。
- 监控效果状态，防范未知和可能的风险：大模型存在随机波动的特点，A版本模型可以顺利使用的Prompt，可能到B版本就失效了。因此，我们要在每个模型版本上，都针对已使用Prompt进行效果监控评估，防止出现“意料之外”的效果下降。
- 挖掘线上BadCase，优化应用效果：通过数据回流获得线上真实流量中的用户请求与应用返回结果（甚至针对多轮交互场景，跟踪收集每一轮次用户请求-响应数据） ，通过评估找到不符合预期的Case，并分析确定优化的方案。

—--

# 2. 制定评估策略：
一个完整的评估策略需要包括评估指标（例如准确率、用户满意度等）、计分方式（例如三分制、GSB等）、评估规则、评估标签等。

- 评估指标：常见的评估指标如下表所示。
- 评估规则：每个场景都需要定义明确的评估规则，即每个评估指标的打分原则、每个分值代表的含义等，为人工或自动化评估提供参考。
- 评估标签：评估时评估人或评估模型需要通过评估标签来标记选择该分值的原因，通常只有打低分时需要标记。
![评估策略](/images/打分.jpg)

---

# 3. 确定评估的方式：
评估方式是与评估指标相关联的，即使是同一个场景的多个指标，也可能分人工评估和自动化评估两种方式，评估结束后需要将所有指标汇总分
析。
![评估方式](/images/评估方式.jpg)

---

# 4. 构建评估集：
需要考虑来源、组成、难度、量级等多个因素。
- 来源：
![评估集的构建](/images/评估集的构建.jpg)
- 组成：
评估集合要足够分散，覆盖不同的场景分支，评估结果才足够可靠。
- 难度：
确保评估集涵盖了不同难度的数据
- 量级：
评估集条数越多，能覆盖的类型越全，评估可信度越高（评估偏差越小） ，但成本也越高，建议量级如下：
![评估集数量建议](/images/评估集数量建议.jpg)

---

# 5. 选择合适的评估流程
常见的评估任务组织方式有如下两种：
- 分配协作评估：评估集分别分配给多人协作完成，每个Case由一个人进行评估打分，可能需要增加互审及验收环节。